# í•œêµ­ ì–¸ë¡ ì‚¬ ë¯¸ë””ì–´ í”„ë ˆì´ë° ë¶„ì„ê¸° - ê°œë°œ ë§¤ë‰´ì–¼

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#2-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [LangGraph ì›Œí¬í”Œë¡œìš°](#3-langgraph-ì›Œí¬í”Œë¡œìš°)
4. [ëª¨ë“ˆë³„ ìƒì„¸ ë¶„ì„](#4-ëª¨ë“ˆë³„-ìƒì„¸-ë¶„ì„)
5. [ë°ì´í„° í”Œë¡œìš°](#5-ë°ì´í„°-í”Œë¡œìš°)
6. [ìƒíƒœ ê´€ë¦¬](#6-ìƒíƒœ-ê´€ë¦¬)
7. [ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°](#7-ì‹¤ì‹œê°„-ìŠ¤íŠ¸ë¦¬ë°)
8. [API ì—°ë™](#8-api-ì—°ë™)
9. [ë°°í¬ ê°€ì´ë“œ](#9-ë°°í¬-ê°€ì´ë“œ)
10. [í…ŒìŠ¤íŠ¸ ë°©ë²•](#10-í…ŒìŠ¤íŠ¸-ë°©ë²•)
11. [í™•ì¥ ë°©ë²•](#11-í™•ì¥-ë°©ë²•)
12. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#12-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 í•µì‹¬ ëª©í‘œ
- **ì‹¤ì‹œê°„ ë¯¸ë””ì–´ í”„ë ˆì´ë° ë¶„ì„**: í•œêµ­ ì£¼ìš” ì–¸ë¡ ì‚¬ë“¤ì˜ ë™ì¼ ì´ìŠˆì— ëŒ€í•œ ë³´ë„ ê´€ì  ì°¨ì´ ë¶„ì„
- **LangGraph ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°**: 6ë‹¨ê³„ ìë™í™”ëœ ë¶„ì„ íŒŒì´í”„ë¼ì¸
- **ì‹¤ì‹œê°„ ì¶”ì **: ê° ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™©ê³¼ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µ

### 1.2 ê¸°ìˆ  ìŠ¤íƒ
- **Frontend**: Streamlit (ì›¹ ì¸í„°í˜ì´ìŠ¤)
- **Workflow Engine**: LangGraph (ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°)
- **LLM Framework**: LangChain + OpenAI GPT
- **Data Collection**: ë„¤ì´ë²„ ë‰´ìŠ¤ API + RSS í”¼ë“œ
- **Real-time Processing**: Python Generator + Streamlit ì‹¤ì‹œê°„ UI
- **State Management**: TypedDict ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬

### 1.3 í”„ë¡œì íŠ¸ êµ¬ì¡°
```
test_v01/
â”œâ”€â”€ streamlit_app.py           # ë©”ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ streaming_workflow.py      # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬í”Œë¡œìš°
â”œâ”€â”€ news_workflow.py           # LangGraph ê¸°ë°˜ ë‰´ìŠ¤ ë¶„ì„ ì›Œí¬í”Œë¡œìš°
â”œâ”€â”€ workflow_nodes.py          # 6ê°œ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ êµ¬í˜„
â”œâ”€â”€ enhanced_news_fetcher.py   # í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
â”œâ”€â”€ news_fetcher.py           # ë„¤ì´ë²„ ë‰´ìŠ¤ API (ë ˆê±°ì‹œ)
â”œâ”€â”€ news_analyzer.py          # OpenAI ê¸°ë°˜ ë¶„ì„ (ë ˆê±°ì‹œ)
â”œâ”€â”€ report_generator.py       # ë³´ê³ ì„œ ìƒì„± (ë ˆê±°ì‹œ)
â”œâ”€â”€ requirements.txt          # íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ config_example.py         # API ì„¤ì • ì˜ˆì‹œ
â”œâ”€â”€ secrets_example.toml      # Secrets ì„¤ì • ì˜ˆì‹œ
â”œâ”€â”€ deployment_guide.md       # ë°°í¬ ê°€ì´ë“œ
â””â”€â”€ README.md                # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ë ˆì´ì–´ë³„ êµ¬ì¡°

#### ğŸ“± Frontend Layer
- **streamlit_app.py**: ë©”ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤
  - ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
  - ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
  - ê²°ê³¼ ì‹œê°í™”
  - API í‚¤ ê´€ë¦¬

#### âš¡ Streaming Layer  
- **streaming_workflow.py**: ì‹¤ì‹œê°„ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬
  - Generator ê¸°ë°˜ ë‹¨ê³„ë³„ ìŠ¤íŠ¸ë¦¬ë°
  - ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
  - ì¤‘ê°„ ê²°ê³¼ ì¦‰ì‹œ í‘œì‹œ

#### ğŸ§  LangGraph Core
- **news_workflow.py**: StateGraph ì •ì˜
- **workflow_nodes.py**: 6ê°œ ë…¸ë“œ êµ¬í˜„
- **WorkflowState**: TypedDict ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬

#### ğŸ“Š Data Collection
- **enhanced_news_fetcher.py**: í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘
- **ë„¤ì´ë²„ API + RSS í”¼ë“œ**: ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘

#### ğŸ” Analysis
- **OpenAI GPT**: LLM ê¸°ë°˜ ë¶„ì„
- **LangChain**: LLM ì—°ë™ í”„ë ˆì„ì›Œí¬

### 2.2 ë°ì´í„° íë¦„
1. **ì‚¬ìš©ì í‚¤ì›Œë“œ ì…ë ¥** â†’ Streamlit ì¸í„°í˜ì´ìŠ¤
2. **ì‹¤ì‹œê°„ ì›Œí¬í”Œë¡œìš° ì‹œì‘** â†’ StreamingWorkflow
3. **LangGraph ì‹¤í–‰** â†’ NewsAnalysisWorkflow  
4. **6ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰** â†’ WorkflowNodes
5. **ì‹¤ì‹œê°„ ê²°ê³¼ ë°˜í™˜** â†’ Generator ìŠ¤íŠ¸ë¦¬ë°
6. **UI ì—…ë°ì´íŠ¸** â†’ Streamlit ì‹¤ì‹œê°„ í‘œì‹œ

---

## 3. LangGraph ì›Œí¬í”Œë¡œìš°

### 3.1 ì›Œí¬í”Œë¡œìš° êµ¬ì¡° (StateGraph)

```python
# news_workflow.pyì˜ í•µì‹¬ êµ¬ì¡°
workflow = StateGraph(WorkflowState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("decide_publishers", self.nodes.decide_publishers)
workflow.add_node("collect_articles", self.nodes.collect_articles)
workflow.add_node("analyze_articles", self.nodes.analyze_articles)
workflow.add_node("compare_analysis", self.nodes.compare_analysis)
workflow.add_node("generate_report", self.nodes.generate_report)
workflow.add_node("suggest_usage", self.nodes.suggest_usage)

# ì—£ì§€ ì •ì˜ (ì„ í˜• ì›Œí¬í”Œë¡œìš°)
workflow.set_entry_point("decide_publishers")
workflow.add_edge("decide_publishers", "collect_articles")
workflow.add_edge("collect_articles", "analyze_articles")
workflow.add_edge("analyze_articles", "compare_analysis")
workflow.add_edge("compare_analysis", "generate_report")
workflow.add_edge("generate_report", "suggest_usage")
workflow.add_edge("suggest_usage", END)
```

### 3.2 ê° ë…¸ë“œë³„ ì—­í• 

#### ğŸ¯ Node 1: decide_publishers
- **ì…ë ¥**: keyword
- **ì²˜ë¦¬**: LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œì— ìµœì í™”ëœ ì–¸ë¡ ì‚¬ ì„ íƒ
- **ì¶œë ¥**: selected_publishers (List[str])
- **LLM í”„ë¡¬í”„íŠ¸**: í‚¤ì›Œë“œ ë¶„ì„ â†’ ì •ì¹˜ì  ì„±í–¥ ë‹¤ì–‘ì„± ê³ ë ¤ â†’ 4-6ê°œ ì–¸ë¡ ì‚¬ ì„ íƒ

#### ğŸ“° Node 2: collect_articles  
- **ì…ë ¥**: keyword + selected_publishers
- **ì²˜ë¦¬**: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
  - ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ
  - ì–¸ë¡ ì‚¬ë³„ RSS í”¼ë“œ ìˆ˜ì§‘
  - ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
- **ì¶œë ¥**: raw_articles (Dict[str, List[Dict]])

#### ğŸ” Node 3: analyze_articles
- **ì…ë ¥**: raw_articles
- **ì²˜ë¦¬**: ê° ê¸°ì‚¬ë³„ LLM ë¶„ì„
  - ìš”ì•½ (3ì¤„ ì´ë‚´)
  - ì–´ì¡° ë¶„ì„ (ê°ê´€ì /ë¹„íŒì /ì˜¹í˜¸ì /ì¤‘ë¦½ì )
  - ê°ì • ë¶„ì„ (ê¸ì •ì /ì¤‘ë¦½ì /ë¶€ì •ì )
  - ì£¼ìš” ë…¼ì  ì¶”ì¶œ
  - í‚¤ì›Œë“œ ì¶”ì¶œ
- **ì¶œë ¥**: analyzed_articles (Dict[str, List[Dict]])

#### ğŸ“Š Node 4: compare_analysis
- **ì…ë ¥**: analyzed_articles  
- **ì²˜ë¦¬**: ì–¸ë¡ ì‚¬ê°„ ë¹„êµ ë¶„ì„
  - ê°ì • ë¶„í¬ ë¹„êµ
  - í”„ë ˆì´ë° ì°¨ì´ì  ë¶„ì„
  - ì–´ì¡° ë¹„êµ
  - ë…¼ì  ì°¨ì´ ë¶„ì„
- **ì¶œë ¥**: comparison_analysis (Dict[str, Any])

#### ğŸ“„ Node 5: generate_report
- **ì…ë ¥**: ëª¨ë“  ì´ì „ ê²°ê³¼
- **ì²˜ë¦¬**: ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
  - ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³´ê³ ì„œ
  - ê°ì • ë¶„í¬ ìš”ì•½
  - í”„ë ˆì´ë° ì°¨ì´ì  ì •ë¦¬
  - ì¢…í•© ë¶„ì„ ê²°ê³¼
- **ì¶œë ¥**: final_report (str)

#### ğŸ’¡ Node 6: suggest_usage
- **ì…ë ¥**: final_report
- **ì²˜ë¦¬**: ë¶„ì„ ê²°ê³¼ í™œìš© ë°©ì•ˆ ì œì•ˆ
  - í•™ìˆ  ì—°êµ¬ í™œìš©ë²•
  - ë¯¸ë””ì–´ ë¦¬í„°ëŸ¬ì‹œ êµìœ¡
  - ì •ì±… ê²°ì • ì°¸ê³  ìë£Œ
- **ì¶œë ¥**: usage_suggestions (List[str])

---

## 4. ëª¨ë“ˆë³„ ìƒì„¸ ë¶„ì„

### 4.1 workflow_nodes.py

#### í´ë˜ìŠ¤ êµ¬ì¡°
```python
class WorkflowState(TypedDict):
    keyword: str
    selected_publishers: List[str]
    raw_articles: Dict[str, List[Dict[str, Any]]]
    analyzed_articles: Dict[str, List[Dict[str, Any]]]
    comparison_analysis: Dict[str, Any]
    final_report: str
    usage_suggestions: List[str]

class NewsWorkflowNodes:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)
        self.enhanced_news_api = EnhancedNewsAPI()
        self.all_publishers = ['ì¡°ì„ ì¼ë³´', 'ë™ì•„ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 
                              'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'SBS', 'MBC', 'KBS']
```

#### í•µì‹¬ ë©”ì„œë“œ ë¶„ì„

**decide_publishers ë©”ì„œë“œ**:
```python
def decide_publishers(self, state: WorkflowState) -> WorkflowState:
    # 1. í‚¤ì›Œë“œ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    # 2. LLM í˜¸ì¶œí•˜ì—¬ ìµœì  ì–¸ë¡ ì‚¬ ì„ íƒ
    # 3. JSON íŒŒì‹±í•˜ì—¬ ì–¸ë¡ ì‚¬ ëª©ë¡ ì¶”ì¶œ
    # 4. ìƒíƒœ ì—…ë°ì´íŠ¸
```

**collect_articles ë©”ì„œë“œ**:
```python
def collect_articles(self, state: WorkflowState) -> WorkflowState:
    # 1. enhanced_news_api.collect_articles_hybrid() í˜¸ì¶œ
    # 2. í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ (ë„¤ì´ë²„ API + RSS)
    # 3. ì–¸ë¡ ì‚¬ë³„ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
    # 4. ìµœëŒ€ 3ê°œ ê¸°ì‚¬ë¡œ ì œí•œ
    # 5. ìƒíƒœ ì—…ë°ì´íŠ¸
```

### 4.2 enhanced_news_fetcher.py

#### í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ì „ëµ
```python
class EnhancedNewsAPI:
    def __init__(self):
        # ì–¸ë¡ ì‚¬ë³„ RSS í”¼ë“œì™€ í‚¤ì›Œë“œ ë§¤í•‘
        self.media_sources = {
            'ì¡°ì„ ì¼ë³´': {
                'rss': 'https://www.chosun.com/arc/outboundfeeds/rss/',
                'keywords': ['ì¡°ì„ ì¼ë³´', 'chosun', 'ì¡°ì„ ']
            },
            # ... ê¸°íƒ€ ì–¸ë¡ ì‚¬
        }
```

#### ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤
1. **ë„¤ì´ë²„ API í˜¸ì¶œ**: ê¸°ë³¸ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
2. **RSS í”¼ë“œ ìˆ˜ì§‘**: ì–¸ë¡ ì‚¬ë³„ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘
3. **ì–¸ë¡ ì‚¬ë³„ í•„í„°ë§**: í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë¶„ë¥˜
4. **ì¤‘ë³µ ì œê±°**: ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
5. **í’ˆì§ˆ ê²€ì¦**: ìµœì†Œ ê¸¸ì´ ë° ë‚´ìš© ê²€ì¦

### 4.3 streaming_workflow.py

#### ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° êµ¬ì¡°
```python
class StreamingWorkflow:
    def run_streaming_analysis(self, keyword: str) -> Generator[Dict[str, Any], None, None]:
        # ìƒíƒœ ì´ˆê¸°í™”
        state = {...}
        
        # ê° ë‹¨ê³„ë³„ ìŠ¤íŠ¸ë¦¬ë°
        for step in self.steps:
            # ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
            yield {"type": "step_start", "step": step["name"], ...}
            
            # ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì‹¤í–‰
            if step["name"] == "decide_publishers":
                state = self.workflow.nodes.decide_publishers(state)
            
            # ë‹¨ê³„ ì™„ë£Œ ì•Œë¦¼ (ê²°ê³¼ í¬í•¨)
            yield {
                "type": "step_complete",
                "step": step["name"],
                "state": state,
                "step_data": self._get_step_data(step["name"], state)
            }
```

#### ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° í˜•ì‹
```python
{
    "type": "step_complete",
    "step": "collect_articles", 
    "message": "âœ… ì´ 10ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ",
    "progress": 35,
    "state": {...},
    "step_data": {
        "articles_by_publisher": {"ì¡°ì„ ì¼ë³´": 3, "í•œê²¨ë ˆ": 2},
        "total_articles": 10,
        "data_sources": {"naver_api": 6, "rss": 4}
    }
}
```

### 4.4 streamlit_app.py

#### ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
```python
def run_streaming_analysis(keyword, workflow_status, step_details):
    # 1. ì§„í–‰ ìƒí™© ì»¨í…Œì´ë„ˆ ìƒì„±
    main_progress = st.progress(0)
    status_container = st.container()
    results_container = st.container()
    
    # 2. ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    for update in streaming_workflow.run_streaming_analysis(keyword):
        # 3. ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
        main_progress.progress(update["progress"])
        status_text.markdown(f"### {update['message']}")
        
        # 4. ë‹¨ê³„ë³„ ê²°ê³¼ ì¦‰ì‹œ í‘œì‹œ
        if update["type"] == "step_complete":
            display_step_result(container, step_name, step_data, state)
```

---

## 5. ë°ì´í„° í”Œë¡œìš°

### 5.1 ìƒíƒœ ë³€í™” ì¶”ì 

#### ì´ˆê¸° ìƒíƒœ
```python
initial_state = {
    "keyword": "ì‚¬ìš©ìì…ë ¥í‚¤ì›Œë“œ",
    "selected_publishers": [],
    "raw_articles": {},
    "analyzed_articles": {},
    "comparison_analysis": {},
    "final_report": "",
    "usage_suggestions": []
}
```

#### ë‹¨ê³„ë³„ ìƒíƒœ ë³€í™”

**Step 1 í›„ ìƒíƒœ**:
```python
{
    "keyword": "ëŒ€í†µë ¹",
    "selected_publishers": ["ì¡°ì„ ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸"],
    "raw_articles": {},
    # ... ë‚˜ë¨¸ì§€ëŠ” ë™ì¼
}
```

**Step 2 í›„ ìƒíƒœ**:
```python
{
    "keyword": "ëŒ€í†µë ¹",
    "selected_publishers": ["ì¡°ì„ ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸"],
    "raw_articles": {
        "ì¡°ì„ ì¼ë³´": [
            {
                "title": "ëŒ€í†µë ¹ ë°œì–¸ ê´€ë ¨ ê¸°ì‚¬",
                "description": "ê¸°ì‚¬ ë‚´ìš©...",
                "link": "https://...",
                "source": "naver_api"
            }
        ],
        "í•œê²¨ë ˆ": [...]
    },
    # ... ë‚˜ë¨¸ì§€
}
```

### 5.2 ë°ì´í„° ë³€í™˜ ê³¼ì •

#### ì›ì‹œ ê¸°ì‚¬ â†’ ë¶„ì„ëœ ê¸°ì‚¬
```python
# ì…ë ¥ (raw_articles)
{
    "title": "ëŒ€í†µë ¹, ìƒˆë¡œìš´ ì •ì±… ë°œí‘œ",
    "description": "ëŒ€í†µë ¹ì´ ì˜¤ëŠ˜ ì²­ì™€ëŒ€ì—ì„œ..."
}

# LLM ë¶„ì„ í›„ (analyzed_articles)  
{
    "title": "ëŒ€í†µë ¹, ìƒˆë¡œìš´ ì •ì±… ë°œí‘œ",
    "description": "ëŒ€í†µë ¹ì´ ì˜¤ëŠ˜ ì²­ì™€ëŒ€ì—ì„œ...",
    "summary": "ëŒ€í†µë ¹ì´ ìƒˆë¡œìš´ ê²½ì œì •ì±…ì„ ë°œí‘œí–ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€...",
    "tone": "ì¤‘ë¦½ì ",
    "sentiment": "ê¸ì •ì ", 
    "main_argument": "ì •ë¶€ì˜ ì ê·¹ì ì¸ ê²½ì œì •ì±… ì¶”ì§„",
    "keywords": ["ëŒ€í†µë ¹", "ì •ì±…", "ê²½ì œ", "ë°œí‘œ"]
}
```

---

## 6. ìƒíƒœ ê´€ë¦¬

### 6.1 TypedDict ê¸°ë°˜ ìƒíƒœ ì •ì˜

```python
from typing import TypedDict, List, Dict, Any

class WorkflowState(TypedDict):
    keyword: str                                    # ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ
    selected_publishers: List[str]                  # ì„ íƒëœ ì–¸ë¡ ì‚¬ ëª©ë¡
    raw_articles: Dict[str, List[Dict[str, Any]]]   # ì–¸ë¡ ì‚¬ë³„ ì›ì‹œ ê¸°ì‚¬
    analyzed_articles: Dict[str, List[Dict[str, Any]]]  # ì–¸ë¡ ì‚¬ë³„ ë¶„ì„ëœ ê¸°ì‚¬
    comparison_analysis: Dict[str, Any]             # ì–¸ë¡ ì‚¬ê°„ ë¹„êµ ë¶„ì„
    final_report: str                               # ìµœì¢… ë³´ê³ ì„œ
    usage_suggestions: List[str]                    # í™œìš© ë°©ì•ˆ ì œì•ˆ
```

### 6.2 ìƒíƒœ ë¶ˆë³€ì„± ê´€ë¦¬

#### LangGraphì˜ ìƒíƒœ ê´€ë¦¬ ì›ì¹™
- ê° ë…¸ë“œëŠ” ìƒˆë¡œìš´ ìƒíƒœ ê°ì²´ë¥¼ ë°˜í™˜
- ì´ì „ ìƒíƒœëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ (ë¶ˆë³€ì„±)
- ìƒíƒœ ì—…ë°ì´íŠ¸ëŠ” ë”•ì…”ë„ˆë¦¬ ë³‘í•© ë°©ì‹

```python
def decide_publishers(self, state: WorkflowState) -> WorkflowState:
    # ê¸°ì¡´ ìƒíƒœ ë³µì‚¬
    new_state = state.copy()
    
    # ìƒˆë¡œìš´ í•„ë“œ ì—…ë°ì´íŠ¸
    new_state["selected_publishers"] = selected_publishers
    
    # ìƒˆë¡œìš´ ìƒíƒœ ë°˜í™˜
    return new_state
```

### 6.3 ìƒíƒœ ê²€ì¦

#### ìƒíƒœ ìœ íš¨ì„± ê²€ì‚¬
```python
def validate_state(state: WorkflowState) -> bool:
    required_fields = ["keyword", "selected_publishers", "raw_articles"]
    return all(field in state for field in required_fields)
```

---

## 7. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

### 7.1 Generator ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°

#### ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬í”Œë¡œìš° êµ¬ì¡°
```python
def run_streaming_analysis(self, keyword: str) -> Generator[Dict[str, Any], None, None]:
    # ìƒíƒœ ì´ˆê¸°í™”
    state = {...}
    
    # ê° ë‹¨ê³„ë³„ ìŠ¤íŠ¸ë¦¬ë°
    for step in self.steps:
        # ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
        yield {"type": "step_start", "step": step["name"], ...}
        
        # ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì‹¤í–‰
        if step["name"] == "decide_publishers":
            state = self.workflow.nodes.decide_publishers(state)
        
        # ë‹¨ê³„ ì™„ë£Œ ì•Œë¦¼ (ê²°ê³¼ í¬í•¨)
        yield {
            "type": "step_complete",
            "step": step["name"],
            "state": state,
            "step_data": self._get_step_data(step["name"], state)
        }
```

### 7.2 ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸

#### Streamlit ì‹¤ì‹œê°„ ì»´í¬ë„ŒíŠ¸
```python
# ì§„í–‰ë¥  ë°”
main_progress = st.progress(0)

# ìƒíƒœ í…ìŠ¤íŠ¸
status_text = st.empty()

# ê²°ê³¼ ì»¨í…Œì´ë„ˆ  
results_container = st.container()

# ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
for update in streaming_workflow.run_streaming_analysis(keyword):
    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
    main_progress.progress(update["progress"])
    
    # ìƒíƒœ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
    status_text.markdown(f"### {update['message']}")
    
    # ë‹¨ê³„ë³„ ê²°ê³¼ ì¦‰ì‹œ í‘œì‹œ
    if update["type"] == "step_complete":
        with results_container:
            display_step_result(update["step"], update["step_data"])
```

### 7.3 ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ íƒ€ì…

#### ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì²˜ë¦¬
```python
EVENT_TYPES = {
    "start": "ë¶„ì„ ì‹œì‘",
    "step_start": "ë‹¨ê³„ ì‹œì‘", 
    "step_running": "ë‹¨ê³„ ì‹¤í–‰ ì¤‘",
    "step_complete": "ë‹¨ê³„ ì™„ë£Œ",
    "step_error": "ë‹¨ê³„ ì˜¤ë¥˜",
    "complete": "ì „ì²´ ì™„ë£Œ"
}
```

---

## 8. API ì—°ë™

### 8.1 OpenAI API ì—°ë™

#### LangChain ê¸°ë°˜ LLM ì„¤ì •
```python
from langchain_openai import ChatOpenAI

class NewsWorkflowNodes:
    def __init__(self):
        # API í‚¤ ì•ˆì „í•œ ê°€ì ¸ì˜¤ê¸°
        api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.3,      # ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•œ ë‚®ì€ temperature
            api_key=api_key
        )
```

#### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
```python
def analyze_article_prompt(title: str, description: str) -> str:
    return f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ë‚´ìš©: {description}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

ìš”ì•½: [3ì¤„ ì´ë‚´ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½]
ì–´ì¡°: [ê°ê´€ì /ë¹„íŒì /ì˜¹í˜¸ì /ì¤‘ë¦½ì  ì¤‘ í•˜ë‚˜]
ê°ì •: [ê¸ì •ì /ì¤‘ë¦½ì /ë¶€ì •ì  ì¤‘ í•˜ë‚˜]  
ì£¼ìš”ë…¼ì : [ì´ ê¸°ì‚¬ê°€ ê°•ì¡°í•˜ëŠ” í•µì‹¬ ì£¼ì¥ì´ë‚˜ ê´€ì ]
í‚¤ì›Œë“œ: [ê¸°ì‚¬ì˜ í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„]
"""
```

### 8.2 ë„¤ì´ë²„ ë‰´ìŠ¤ API ì—°ë™

#### API í˜¸ì¶œ êµ¬ì¡°
```python
def _get_naver_articles(self, keyword: str) -> List[Dict[str, Any]]:
    headers = {
        'X-Naver-Client-Id': self.naver_client_id,
        'X-Naver-Client-Secret': self.naver_client_secret
    }
    
    params = {
        'query': keyword,
        'display': 100,
        'start': 1,
        'sort': 'date'
    }
    
    response = requests.get(
        "https://openapi.naver.com/v1/search/news.json",
        headers=headers,
        params=params,
        timeout=10
    )
```

#### API ì‘ë‹µ ì²˜ë¦¬
```python
def _filter_naver_articles(self, articles: List[Dict], publishers: List[str]) -> Dict[str, List[Dict]]:
    filtered = {pub: [] for pub in publishers}
    
    for article in articles:
        # HTML íƒœê·¸ ì œê±°
        title = self._clean_html(article.get('title', ''))
        
        # ì–¸ë¡ ì‚¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        for publisher in publishers:
            keywords = self.media_sources[publisher]['keywords']
            if any(keyword.lower() in title.lower() for keyword in keywords):
                filtered[publisher].append({
                    'title': title,
                    'description': self._clean_html(article.get('description', '')),
                    'link': article.get('originallink', ''),
                    'source': 'naver_api'
                })
                break
    
    return filtered
```

### 8.3 RSS í”¼ë“œ ì—°ë™

#### RSS íŒŒì‹±
```python
import feedparser

def _get_rss_articles(self, publisher: str, keyword: str) -> List[Dict[str, Any]]:
    rss_url = self.media_sources[publisher]['rss']
    
    response = requests.get(rss_url, timeout=10)
    feed = feedparser.parse(response.content)
    
    articles = []
    for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œë§Œ
        if keyword.lower() in entry.title.lower() or keyword.lower() in entry.summary.lower():
            articles.append({
                'title': entry.title,
                'description': entry.summary,
                'link': entry.link,
                'pubDate': entry.published if hasattr(entry, 'published') else '',
                'source': 'rss'
            })
    
    return articles
```

---

## 9. ë°°í¬ ê°€ì´ë“œ

### 9.1 Streamlit Cloud ë°°í¬

#### 1ë‹¨ê³„: ë¦¬í¬ì§€í† ë¦¬ ì¤€ë¹„
```bash
git add .
git commit -m "Initial commit"
git push origin main
```

#### 2ë‹¨ê³„: Streamlit Cloud ì„¤ì •
1. https://share.streamlit.io ì ‘ì†
2. New app í´ë¦­
3. GitHub ë¦¬í¬ì§€í† ë¦¬ ì—°ê²°
4. Main file path: `streamlit_app.py`
5. Deploy í´ë¦­

#### 3ë‹¨ê³„: Secrets ì„¤ì •
App Settings â†’ Secretsì—ì„œ:
```toml
NAVER_CLIENT_ID = "ì‹¤ì œ_í´ë¼ì´ì–¸íŠ¸_ID"
NAVER_CLIENT_SECRET = "ì‹¤ì œ_ì‹œí¬ë¦¿"
OPENAI_API_KEY = "ì‹¤ì œ_OpenAI_í‚¤"
```

### 9.2 ë¡œì»¬ ê°œë°œ í™˜ê²½

#### í™˜ê²½ ì„¤ì •
```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 3. Secrets ì„¤ì •
mkdir .streamlit
echo '[default]
NAVER_CLIENT_ID = "your_id"
NAVER_CLIENT_SECRET = "your_secret"  
OPENAI_API_KEY = "your_key"' > .streamlit/secrets.toml

# 4. ì‹¤í–‰
streamlit run streamlit_app.py
```

### 9.3 í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì •

#### ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜
```bash
export NAVER_CLIENT_ID="your_client_id"
export NAVER_CLIENT_SECRET="your_client_secret"
export OPENAI_API_KEY="your_openai_key"
```

#### ì½”ë“œì—ì„œ í™˜ê²½ ê°ì§€
```python
def get_api_keys():
    try:
        # Streamlit Cloud í™˜ê²½
        if hasattr(st, 'secrets') and len(st.secrets) > 0:
            return (
                st.secrets.get("NAVER_CLIENT_ID", ""),
                st.secrets.get("NAVER_CLIENT_SECRET", ""), 
                st.secrets.get("OPENAI_API_KEY", "")
            )
        else:
            # ë¡œì»¬ í™˜ê²½
            return (
                os.getenv("NAVER_CLIENT_ID", ""),
                os.getenv("NAVER_CLIENT_SECRET", ""),
                os.getenv("OPENAI_API_KEY", "")
            )
    except Exception:
        return "", "", ""
```

---

## 10. í…ŒìŠ¤íŠ¸ ë°©ë²•

### 10.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

#### ì›Œí¬í”Œë¡œìš° ë…¸ë“œ í…ŒìŠ¤íŠ¸
```python
# test_workflow_nodes.py ìƒì„± ì˜ˆì‹œ
import unittest
from workflow_nodes import NewsWorkflowNodes, WorkflowState

class TestWorkflowNodes(unittest.TestCase):
    def setUp(self):
        self.nodes = NewsWorkflowNodes()
        self.test_state = {
            "keyword": "í…ŒìŠ¤íŠ¸",
            "selected_publishers": [],
            "raw_articles": {},
            "analyzed_articles": {},
            "comparison_analysis": {},
            "final_report": "",
            "usage_suggestions": []
        }
    
    def test_decide_publishers(self):
        result = self.nodes.decide_publishers(self.test_state)
        self.assertIn("selected_publishers", result)
        self.assertIsInstance(result["selected_publishers"], list)
        
    def test_collect_articles(self):
        # ì–¸ë¡ ì‚¬ê°€ ì„ íƒëœ ìƒíƒœë¡œ í…ŒìŠ¤íŠ¸
        self.test_state["selected_publishers"] = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ"]
        result = self.nodes.collect_articles(self.test_state)
        self.assertIn("raw_articles", result)
```

#### ì‹¤í–‰ ë°©ë²•
```bash
python -m pytest test_workflow_nodes.py -v
```

### 10.2 í†µí•© í…ŒìŠ¤íŠ¸

#### ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
```python
def test_full_workflow():
    from news_workflow import run_news_analysis
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œë¡œ í…ŒìŠ¤íŠ¸
    result = run_news_analysis("ê²½ì œ")
    
    # ê²°ê³¼ ê²€ì¦
    assert "final_report" in result
    assert "usage_suggestions" in result
    assert len(result["selected_publishers"]) > 0
```

### 10.3 API ì—°ë™ í…ŒìŠ¤íŠ¸

#### Mockì„ ì‚¬ìš©í•œ API í…ŒìŠ¤íŠ¸
```python
from unittest.mock import patch, MagicMock

@patch('enhanced_news_fetcher.requests.get')
def test_naver_api(mock_get):
    # Mock ì‘ë‹µ ì„¤ì •
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        'items': [{'title': 'í…ŒìŠ¤íŠ¸ ê¸°ì‚¬', 'description': 'ë‚´ìš©'}]
    }
    mock_get.return_value = mock_response
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    api = EnhancedNewsAPI()
    articles = api._get_naver_articles("í…ŒìŠ¤íŠ¸")
    
    # ê²€ì¦
    assert len(articles) > 0
```

---

## 11. í™•ì¥ ë°©ë²•

### 11.1 ìƒˆë¡œìš´ ì–¸ë¡ ì‚¬ ì¶”ê°€

#### enhanced_news_fetcher.py ìˆ˜ì •
```python
# media_sourcesì— ìƒˆë¡œìš´ ì–¸ë¡ ì‚¬ ì¶”ê°€
self.media_sources['ìƒˆë¡œìš´ì–¸ë¡ ì‚¬'] = {
    'rss': 'https://ìƒˆë¡œìš´ì–¸ë¡ ì‚¬.com/rss.xml',
    'website': 'https://ìƒˆë¡œìš´ì–¸ë¡ ì‚¬.com',
    'search_url': 'https://ìƒˆë¡œìš´ì–¸ë¡ ì‚¬.com/search/{keyword}',
    'keywords': ['ìƒˆë¡œìš´ì–¸ë¡ ì‚¬', 'new_media', 'ìƒˆë¡œìš´']
}
```

#### workflow_nodes.py ìˆ˜ì •
```python
# all_publishers ëª©ë¡ì— ì¶”ê°€
self.all_publishers = ['ì¡°ì„ ì¼ë³´', 'ë™ì•„ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 
                      'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'SBS', 'MBC', 'KBS', 'ìƒˆë¡œìš´ì–¸ë¡ ì‚¬']
```

### 11.2 ìƒˆë¡œìš´ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€

#### ê°ì • ë¶„ì„ ê³ ë„í™”
```python
def enhanced_sentiment_analysis(self, text: str) -> Dict[str, float]:
    """ê°ì • ì ìˆ˜ë¥¼ 0-1 ì‚¬ì´ ì‹¤ìˆ˜ë¡œ ë°˜í™˜"""
    prompt = f"""
    ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ 0-1 ì‚¬ì´ ì ìˆ˜ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
    í…ìŠ¤íŠ¸: {text}
    
    ì‘ë‹µ í˜•ì‹:
    ê¸ì •ì : 0.7
    ì¤‘ë¦½ì : 0.2  
    ë¶€ì •ì : 0.1
    """
    # LLM í˜¸ì¶œ ë° íŒŒì‹± ë¡œì§
```

#### ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì¶”ê°€
```python
def sentiment_visualization(self, state: WorkflowState) -> WorkflowState:
    """ê°ì • ë¶„ì„ ì‹œê°í™” ë°ì´í„° ìƒì„±"""
    analyzed_articles = state["analyzed_articles"]
    
    # ì‹œê°í™”ìš© ë°ì´í„° ìƒì„±
    viz_data = {}
    for publisher, articles in analyzed_articles.items():
        sentiments = [article['sentiment'] for article in articles]
        viz_data[publisher] = {
            'positive_count': sentiments.count('ê¸ì •ì '),
            'neutral_count': sentiments.count('ì¤‘ë¦½ì '),
            'negative_count': sentiments.count('ë¶€ì •ì ')
        }
    
    state["visualization_data"] = viz_data
    return state
```

### 11.3 ë‹¤ë¥¸ LLM ëª¨ë¸ ì—°ë™

#### Anthropic Claude ì—°ë™
```python
from langchain_anthropic import ChatAnthropic

class MultiLLMWorkflowNodes(NewsWorkflowNodes):
    def __init__(self):
        super().__init__()
        self.claude = ChatAnthropic(
            model="claude-3-sonnet-20240229",
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
    
    def analyze_with_multiple_llms(self, text: str):
        # OpenAI ë¶„ì„
        openai_result = self.llm.invoke([HumanMessage(content=text)])
        
        # Claude ë¶„ì„
        claude_result = self.claude.invoke([HumanMessage(content=text)])
        
        # ê²°ê³¼ ë¹„êµ ë° í†µí•©
        return self.combine_llm_results(openai_result, claude_result)
```

### 11.4 ì‹¤ì‹œê°„ ì•Œë¦¼ ê¸°ëŠ¥

#### Slack/Discord ì—°ë™
```python
import requests

def send_analysis_notification(keyword: str, results: Dict):
    """ë¶„ì„ ì™„ë£Œì‹œ Slackìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡"""
    webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    
    message = {
        "text": f"ğŸ” '{keyword}' í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ!",
        "attachments": [
            {
                "fields": [
                    {"title": "ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜", "value": str(total_articles), "short": True},
                    {"title": "ì–¸ë¡ ì‚¬ ìˆ˜", "value": str(len(results["selected_publishers"])), "short": True}
                ]
            }
        ]
    }
    
    requests.post(webhook_url, json=message)
```

---

## 12. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 12.1 ì¼ë°˜ì ì¸ ë¬¸ì œ

#### API í‚¤ ê´€ë ¨ ì˜¤ë¥˜
```
Error: OpenAI API key not found
```
**í•´ê²°ì±…**:
1. Streamlit Cloud Secrets í™•ì¸
2. ë¡œì»¬ .streamlit/secrets.toml í™•ì¸
3. í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸

#### ë„¤ì´ë²„ API í• ë‹¹ëŸ‰ ì´ˆê³¼
```
Error: Quota exceeded for Naver API
```
**í•´ê²°ì±…**:
1. API ì‚¬ìš©ëŸ‰ í™•ì¸ (developers.naver.com)
2. RSS í”¼ë“œë§Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“œë¡œ ì „í™˜
3. API í‚¤ ì—…ê·¸ë ˆì´ë“œ

#### LangGraph ìƒíƒœ ê´€ë¦¬ ì˜¤ë¥˜
```
Error: StateGraph validation failed
```
**í•´ê²°ì±…**:
1. WorkflowState TypedDict í™•ì¸
2. ë…¸ë“œ ë°˜í™˜ê°’ íƒ€ì… ê²€ì¦
3. ìƒíƒœ í•„ë“œëª… ì¼ì¹˜ ì—¬ë¶€ í™•ì¸

### 12.2 ì„±ëŠ¥ ìµœì í™”

#### LLM í˜¸ì¶œ ìµœì í™”
```python
# ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ íšŸìˆ˜ ì¤„ì´ê¸°
def analyze_articles_batch(self, articles: List[Dict]) -> List[Dict]:
    batch_prompt = "\n\n---\n\n".join([
        f"ê¸°ì‚¬ {i+1}:\nì œëª©: {article['title']}\në‚´ìš©: {article['description']}"
        for i, article in enumerate(articles)
    ])
    
    # í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ì—¬ëŸ¬ ê¸°ì‚¬ ë¶„ì„
    response = self.llm.invoke([HumanMessage(content=batch_prompt)])
    
    # ì‘ë‹µ íŒŒì‹±í•˜ì—¬ ê°œë³„ ê¸°ì‚¬ ê²°ê³¼ë¡œ ë¶„ë¦¬
    return self.parse_batch_response(response.content, len(articles))
```

#### ìºì‹± êµ¬í˜„
```python
import streamlit as st

@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def cached_article_analysis(title: str, description: str) -> Dict:
    """ë¶„ì„ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€"""
    return analyze_article(title, description)
```

### 12.3 ì—ëŸ¬ í•¸ë“¤ë§

#### ê²¬ê³ í•œ ì—ëŸ¬ ì²˜ë¦¬
```python
def robust_llm_call(self, prompt: str, max_retries: int = 3) -> str:
    """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ì•ˆì „í•œ LLM í˜¸ì¶œ"""
    for attempt in range(max_retries):
        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            return response.content
        except Exception as e:
            if attempt == max_retries - 1:
                return f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    return "ë¶„ì„ ì‹¤íŒ¨: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼"
```

#### ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬
```python
def partial_failure_handling(self, state: WorkflowState) -> WorkflowState:
    """ì¼ë¶€ ì–¸ë¡ ì‚¬ì—ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨ì‹œì—ë„ ê³„ì† ì§„í–‰"""
    successful_publishers = []
    failed_publishers = []
    
    for publisher in state["selected_publishers"]:
        try:
            articles = self.collect_from_publisher(publisher, state["keyword"])
            if articles:
                state["raw_articles"][publisher] = articles
                successful_publishers.append(publisher)
            else:
                failed_publishers.append(publisher)
        except Exception as e:
            failed_publishers.append(publisher)
            print(f"âŒ {publisher} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # ì‹¤íŒ¨í•œ ì–¸ë¡ ì‚¬ëŠ” ì œì™¸í•˜ê³  ê³„ì† ì§„í–‰
    state["selected_publishers"] = successful_publishers
    
    if failed_publishers:
        print(f"âš ï¸ ìˆ˜ì§‘ ì‹¤íŒ¨í•œ ì–¸ë¡ ì‚¬: {', '.join(failed_publishers)}")
    
    return state
```

### 12.4 ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

#### ìƒì„¸ ë¡œê¹…
```python
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('analysis.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def log_workflow_progress(self, step_name: str, state: WorkflowState):
    """ì›Œí¬í”Œë¡œìš° ì§„í–‰ ìƒí™© ë¡œê¹…"""
    logger.info(f"Step {step_name} started for keyword: {state['keyword']}")
    logger.info(f"Current publishers: {state['selected_publishers']}")
    logger.info(f"Articles collected: {sum(len(articles) for articles in state['raw_articles'].values())}")
```

---

## ğŸ“ ê°œë°œ ì§€ì›

### ë¬¸ì˜ ë° ê¸°ì—¬
- **Issues**: GitHub Issuesì—ì„œ ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ìš”ì²­
- **Pull Requests**: ì½”ë“œ ê¸°ì—¬ëŠ” PRë¡œ ì œì¶œ
- **Documentation**: ì¶”ê°€ ë¬¸ì„œëŠ” docs/ í´ë”ì— ì‘ì„±

### ë¼ì´ì„¼ìŠ¤
ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

*ì´ ë§¤ë‰´ì–¼ì€ í”„ë¡œì íŠ¸ì˜ ì§€ì†ì ì¸ ë°œì „ê³¼ í•¨ê»˜ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.* 