# ğŸ“˜ í•œêµ­ ì–¸ë¡ ì‚¬ ë¯¸ë””ì–´ í”„ë ˆì´ë° ë¶„ì„ê¸° - ê°œë°œ ê°€ì´ë“œ

> **LangGraph + MCP ê¸°ë°˜ ì—…ë¬´ìë™í™” ì‹œìŠ¤í…œ**  
> ì‹¤ì‹œê°„ ë‹¨ê³„ë³„ ì¶”ì  & í”¼ë“œë°± ì œê³µ

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

### í•µì‹¬ ê¸°ëŠ¥
- í‚¤ì›Œë“œ ê¸°ë°˜ í•œêµ­ ì–¸ë¡ ì‚¬ ê¸°ì‚¬ ìë™ ìˆ˜ì§‘
- OpenAI GPT ê¸°ë°˜ ì‹¤ì‹œê°„ ë¶„ì„ (ìš”ì•½, ê°ì •, í”„ë ˆì´ë°)  
- ì–¸ë¡ ì‚¬ê°„ ë³´ë„ ê´€ì  ì°¨ì´ ë¹„êµ
- LangGraph ì›Œí¬í”Œë¡œìš°ë¡œ 6ë‹¨ê³„ ìë™í™”
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶”ì  ë° ì¤‘ê°„ ê²°ê³¼ í‘œì‹œ

### ë¶„ì„ ëŒ€ìƒ ì–¸ë¡ ì‚¬
- **ë³´ìˆ˜**: ì¡°ì„ ì¼ë³´, ë™ì•„ì¼ë³´, ì¤‘ì•™ì¼ë³´
- **ì§„ë³´**: í•œê²¨ë ˆ, ê²½í–¥ì‹ ë¬¸  
- **ë°©ì†¡**: SBS, MBC, KBS

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ë ˆì´ì–´ êµ¬ì¡°
```
ğŸ“± Frontend (Streamlit)
    â†“
âš¡ Streaming Layer (ì‹¤ì‹œê°„ ì¶”ì )
    â†“  
ğŸ§  LangGraph Core (6ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°)
    â†“
ğŸ“Š Data Collection (í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘)
    â†“
ğŸŒ External APIs (ë„¤ì´ë²„, OpenAI, RSS)
```

### í•µì‹¬ íŒŒì¼ êµ¬ì¡°
```
streamlit_app.py          # ì›¹ ì¸í„°í˜ì´ìŠ¤ + ì‹¤ì‹œê°„ UI
streaming_workflow.py     # Generator ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°  
news_workflow.py          # LangGraph StateGraph ì •ì˜
workflow_nodes.py         # 6ê°œ ë…¸ë“œ êµ¬í˜„
enhanced_news_fetcher.py  # í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘
```

## ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš°

### 6ë‹¨ê³„ ìë™í™” íŒŒì´í”„ë¼ì¸

1. **ğŸ¯ decide_publishers**: AIê°€ í‚¤ì›Œë“œ ë¶„ì„í•˜ì—¬ ìµœì  ì–¸ë¡ ì‚¬ ì„ íƒ
2. **ğŸ“° collect_articles**: ë„¤ì´ë²„ API + RSS í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘
3. **ğŸ” analyze_articles**: ê° ê¸°ì‚¬ë³„ ìš”ì•½, ê°ì •, ì–´ì¡°, ë…¼ì  ë¶„ì„  
4. **ğŸ“Š compare_analysis**: ì–¸ë¡ ì‚¬ê°„ ì…ì¥ ì°¨ì´ ë¹„êµ
5. **ğŸ“„ generate_report**: ë§ˆí¬ë‹¤ìš´ ì¢…í•© ë³´ê³ ì„œ ìƒì„±
6. **ğŸ’¡ suggest_usage**: ë¶„ì„ ê²°ê³¼ í™œìš© ë°©ì•ˆ ì œì•ˆ

### StateGraph ì •ì˜ ì½”ë“œ
```python
# news_workflow.py
workflow = StateGraph(WorkflowState)

# 6ê°œ ë…¸ë“œ ì¶”ê°€
workflow.add_node("decide_publishers", self.nodes.decide_publishers)
workflow.add_node("collect_articles", self.nodes.collect_articles)
workflow.add_node("analyze_articles", self.nodes.analyze_articles)
workflow.add_node("compare_analysis", self.nodes.compare_analysis)
workflow.add_node("generate_report", self.nodes.generate_report)
workflow.add_node("suggest_usage", self.nodes.suggest_usage)

# ì„ í˜• ì—£ì§€ ì—°ê²°
workflow.set_entry_point("decide_publishers")
workflow.add_edge("decide_publishers", "collect_articles")
workflow.add_edge("collect_articles", "analyze_articles")
workflow.add_edge("analyze_articles", "compare_analysis")
workflow.add_edge("compare_analysis", "generate_report")
workflow.add_edge("generate_report", "suggest_usage")
workflow.add_edge("suggest_usage", END)
```

## ğŸ“Š ìƒíƒœ ê´€ë¦¬ (WorkflowState)

### TypedDict ê¸°ë°˜ ìƒíƒœ ì •ì˜
```python
class WorkflowState(TypedDict):
    keyword: str                                    # ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ
    selected_publishers: List[str]                  # ì„ íƒëœ ì–¸ë¡ ì‚¬
    raw_articles: Dict[str, List[Dict[str, Any]]]   # ìˆ˜ì§‘ëœ ì›ì‹œ ê¸°ì‚¬
    analyzed_articles: Dict[str, List[Dict[str, Any]]]  # ë¶„ì„ëœ ê¸°ì‚¬
    comparison_analysis: Dict[str, Any]             # ë¹„êµ ë¶„ì„ ê²°ê³¼
    final_report: str                               # ìµœì¢… ë³´ê³ ì„œ
    usage_suggestions: List[str]                    # í™œìš© ë°©ì•ˆ
```

### ìƒíƒœ ë³€í™” ì˜ˆì‹œ
```python
# ì´ˆê¸° ìƒíƒœ
{"keyword": "ëŒ€í†µë ¹", "selected_publishers": [], ...}

# Step 1 í›„
{"keyword": "ëŒ€í†µë ¹", "selected_publishers": ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ"], ...}

# Step 2 í›„  
{"keyword": "ëŒ€í†µë ¹", "selected_publishers": [...], 
 "raw_articles": {"ì¡°ì„ ì¼ë³´": [{"title": "...", "description": "..."}], ...}}
```

## âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ

### Generator ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° êµ¬ì¡°
```python
# streaming_workflow.py
def run_streaming_analysis(self, keyword: str) -> Generator[Dict[str, Any], None, None]:
    state = {...}  # ì´ˆê¸° ìƒíƒœ
    
    # ê° ë‹¨ê³„ë³„ ìŠ¤íŠ¸ë¦¬ë°
    for step in self.steps:
        # ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
        yield {"type": "step_start", "step": step["name"], "progress": step["progress"]-5}
        
        # ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì‹¤í–‰  
        state = self.workflow.nodes.method_name(state)
        
        # ë‹¨ê³„ ì™„ë£Œ ì•Œë¦¼ (ê²°ê³¼ í¬í•¨)
        yield {
            "type": "step_complete", 
            "step": step["name"],
            "state": state,
            "step_data": self._get_step_data(step["name"], state)
        }
```

### ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸ (Streamlit)
```python
# streamlit_app.py
def run_streaming_analysis(keyword, workflow_status, step_details):
    main_progress = st.progress(0)
    status_container = st.container()
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
    for update in streaming_workflow.run_streaming_analysis(keyword):
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        main_progress.progress(update["progress"])
        
        # ìƒíƒœ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        status_text.markdown(f"### {update['message']}")
        
        # ë‹¨ê³„ë³„ ê²°ê³¼ ì¦‰ì‹œ í‘œì‹œ
        if update["type"] == "step_complete":
            display_step_result(update["step"], update["step_data"])
```

## ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘

### ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ ì „ëµ
```python
# enhanced_news_fetcher.py
class EnhancedNewsAPI:
    def collect_articles_hybrid(self, keyword: str, publishers: List[str]):
        all_articles = {}
        
        # 1. ë„¤ì´ë²„ API ê¸°ë³¸ ìˆ˜ì§‘
        naver_articles = self._get_naver_articles(keyword)
        naver_filtered = self._filter_naver_articles(naver_articles, publishers)
        
        # 2. ì–¸ë¡ ì‚¬ë³„ RSS í”¼ë“œ ìˆ˜ì§‘
        for publisher in publishers:
            articles = []
            
            # ë„¤ì´ë²„ ê²°ê³¼ ì¶”ê°€
            if publisher in naver_filtered:
                articles.extend(naver_filtered[publisher])
            
            # RSS í”¼ë“œì—ì„œ ì¶”ê°€ ìˆ˜ì§‘  
            rss_articles = self._get_rss_articles(publisher, keyword)
            articles.extend(rss_articles)
            
            # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 3ê°œ ì„ íƒ
            all_articles[publisher] = self._remove_duplicates(articles)[:3]
        
        return all_articles
```

### ì–¸ë¡ ì‚¬ë³„ RSS ë§¤í•‘
```python
self.media_sources = {
    'ì¡°ì„ ì¼ë³´': {
        'rss': 'https://www.chosun.com/arc/outboundfeeds/rss/',
        'keywords': ['ì¡°ì„ ì¼ë³´', 'chosun', 'ì¡°ì„ ']
    },
    'í•œê²¨ë ˆ': {
        'rss': 'http://feeds.hani.co.kr/rss/newsstand/',
        'keywords': ['í•œê²¨ë ˆ', 'hani']
    },
    # ... ê¸°íƒ€ ì–¸ë¡ ì‚¬
}
```

## ğŸ”Œ API ì—°ë™ êµ¬ì¡°

### OpenAI GPT ì—°ë™ (LangChain)
```python
# workflow_nodes.py
class NewsWorkflowNodes:
    def __init__(self):
        # ì•ˆì „í•œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.3,  # ì¼ê´€ëœ ê²°ê³¼
            api_key=api_key
        )
    
    def analyze_article(self, title: str, description: str):
        prompt = f"""
        ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
        ì œëª©: {title}
        ë‚´ìš©: {description}
        
        í˜•ì‹:
        ìš”ì•½: [3ì¤„ ì´ë‚´ ìš”ì•½]
        ì–´ì¡°: [ê°ê´€ì /ë¹„íŒì /ì˜¹í˜¸ì /ì¤‘ë¦½ì ]
        ê°ì •: [ê¸ì •ì /ì¤‘ë¦½ì /ë¶€ì •ì ]
        ì£¼ìš”ë…¼ì : [í•µì‹¬ ì£¼ì¥]
        í‚¤ì›Œë“œ: [í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œ]
        """
        
        response = self.llm.invoke([
            SystemMessage(content="ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€"),
            HumanMessage(content=prompt)
        ])
        
        return self._parse_analysis(response.content)
```

### ë„¤ì´ë²„ ë‰´ìŠ¤ API ì—°ë™
```python
def _get_naver_articles(self, keyword: str) -> List[Dict]:
    headers = {
        'X-Naver-Client-Id': self.naver_client_id,
        'X-Naver-Client-Secret': self.naver_client_secret
    }
    
    params = {
        'query': keyword,
        'display': 100,
        'start': 1, 
        'sort': 'date'
    }
    
    response = requests.get(
        "https://openapi.naver.com/v1/search/news.json",
        headers=headers,
        params=params,
        timeout=10
    )
    
    return response.json().get('items', [])
```

## ğŸš€ ë°°í¬ ê°€ì´ë“œ

### Streamlit Cloud ë°°í¬ (ê¶Œì¥)

#### 1ë‹¨ê³„: ë¦¬í¬ì§€í† ë¦¬ ì¤€ë¹„
```bash
git add .
git commit -m "í”„ë¡œì íŠ¸ ì™„ì„±"
git push origin main
```

#### 2ë‹¨ê³„: Streamlit Cloud ì„¤ì •
1. **https://share.streamlit.io** ì ‘ì†
2. **New app** í´ë¦­
3. **GitHub ë¦¬í¬ì§€í† ë¦¬ ì—°ê²°**
4. **Main file path**: `streamlit_app.py`
5. **Deploy** í´ë¦­

#### 3ë‹¨ê³„: Secrets ì„¤ì • (ì¤‘ìš”!)
App Settings â†’ Secretsì—ì„œ:
```toml
NAVER_CLIENT_ID = "ì‹¤ì œ_ë„¤ì´ë²„_í´ë¼ì´ì–¸íŠ¸_ID"
NAVER_CLIENT_SECRET = "ì‹¤ì œ_ë„¤ì´ë²„_ì‹œí¬ë¦¿"
OPENAI_API_KEY = "ì‹¤ì œ_OpenAI_API_í‚¤"
```

### ë¡œì»¬ ê°œë°œ í™˜ê²½
```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 3. Secrets ì„¤ì •
mkdir .streamlit
cat > .streamlit/secrets.toml << EOF
[default]
NAVER_CLIENT_ID = "your_naver_id"
NAVER_CLIENT_SECRET = "your_naver_secret"
OPENAI_API_KEY = "your_openai_key"
EOF

# 4. ì‹¤í–‰
streamlit run streamlit_app.py
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í´ë” ìƒì„±
```bash
mkdir tests
cd tests
```

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„±
```python
# tests/test_workflow_nodes.py
import unittest
from workflow_nodes import NewsWorkflowNodes, WorkflowState

class TestWorkflowNodes(unittest.TestCase):
    def setUp(self):
        self.nodes = NewsWorkflowNodes()
        self.test_state = {
            "keyword": "í…ŒìŠ¤íŠ¸",
            "selected_publishers": [],
            "raw_articles": {},
            "analyzed_articles": {},
            "comparison_analysis": {},
            "final_report": "",
            "usage_suggestions": []
        }
    
    def test_decide_publishers(self):
        result = self.nodes.decide_publishers(self.test_state)
        self.assertIn("selected_publishers", result)
        self.assertIsInstance(result["selected_publishers"], list)
        self.assertTrue(len(result["selected_publishers"]) > 0)
    
    def test_collect_articles(self):
        self.test_state["selected_publishers"] = ["ì¡°ì„ ì¼ë³´", "í•œê²¨ë ˆ"]
        result = self.nodes.collect_articles(self.test_state)
        self.assertIn("raw_articles", result)
        self.assertIsInstance(result["raw_articles"], dict)

# ì‹¤í–‰: python -m pytest tests/test_workflow_nodes.py -v
```

### í†µí•© í…ŒìŠ¤íŠ¸
```python
# tests/test_integration.py
def test_full_workflow():
    from news_workflow import run_news_analysis
    
    result = run_news_analysis("ê²½ì œ")
    
    # ê²°ê³¼ ê²€ì¦
    assert "final_report" in result
    assert "usage_suggestions" in result
    assert len(result["selected_publishers"]) > 0
    print("âœ… ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ í†µê³¼")

# ì‹¤í–‰: python tests/test_integration.py
```

## ğŸ”§ í™•ì¥ ë°©ë²•

### ìƒˆë¡œìš´ ì–¸ë¡ ì‚¬ ì¶”ê°€

#### enhanced_news_fetcher.py ìˆ˜ì •
```python
# media_sourcesì— ìƒˆ ì–¸ë¡ ì‚¬ ì¶”ê°€
self.media_sources['ìƒˆë¡œìš´ì–¸ë¡ ì‚¬'] = {
    'rss': 'https://ìƒˆë¡œìš´ì–¸ë¡ ì‚¬.com/rss.xml',
    'website': 'https://ìƒˆë¡œìš´ì–¸ë¡ ì‚¬.com',
    'keywords': ['ìƒˆë¡œìš´ì–¸ë¡ ì‚¬', 'newmedia', 'ìƒˆë¡œìš´']
}
```

#### workflow_nodes.py ìˆ˜ì •
```python
# all_publishers ëª©ë¡ì— ì¶”ê°€
self.all_publishers = [
    'ì¡°ì„ ì¼ë³´', 'ë™ì•„ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 
    'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'SBS', 'MBC', 'KBS', 
    'ìƒˆë¡œìš´ì–¸ë¡ ì‚¬'  # ì¶”ê°€
]
```

### ìƒˆë¡œìš´ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€

#### ê°ì • ë¶„ì„ ê³ ë„í™”
```python
def enhanced_sentiment_analysis(self, text: str) -> Dict[str, float]:
    """ê°ì •ì„ 0-1 ì‚¬ì´ ì ìˆ˜ë¡œ ë°˜í™˜"""
    prompt = f"""
    í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ 0-1 ì ìˆ˜ë¡œ ë¶„ì„:
    {text}
    
    í˜•ì‹:
    ê¸ì •ì : 0.7
    ì¤‘ë¦½ì : 0.2
    ë¶€ì •ì : 0.1
    """
    # LLM í˜¸ì¶œ ë° íŒŒì‹±
```

#### ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì¶”ê°€
```python
def keyword_trend_analysis(self, state: WorkflowState) -> WorkflowState:
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ë…¸ë“œ"""
    keyword = state["keyword"]
    
    # íŠ¸ë Œë“œ ë¶„ì„ ë¡œì§
    trend_data = self.analyze_keyword_trends(keyword)
    
    state["trend_analysis"] = trend_data
    return state

# StateGraphì— ë…¸ë“œ ì¶”ê°€
workflow.add_node("keyword_trend", self.nodes.keyword_trend_analysis)
workflow.add_edge("suggest_usage", "keyword_trend")
workflow.add_edge("keyword_trend", END)
```

### ë‹¤ë¥¸ LLM ëª¨ë¸ ì—°ë™
```python
from langchain_anthropic import ChatAnthropic

class MultiLLMNodes(NewsWorkflowNodes):
    def __init__(self):
        super().__init__()
        self.claude = ChatAnthropic(
            model="claude-3-sonnet-20240229",
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
    
    def compare_llm_analysis(self, text: str):
        # OpenAIì™€ Claude ê²°ê³¼ ë¹„êµ
        openai_result = self.llm.invoke([HumanMessage(content=text)])
        claude_result = self.claude.invoke([HumanMessage(content=text)])
        
        return {
            "openai": openai_result.content,
            "claude": claude_result.content,
            "comparison": self.compare_results(openai_result, claude_result)
        }
```

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### API í‚¤ ì˜¤ë¥˜
```
Error: OpenAI API key not found
```
**í•´ê²°ì±…**:
1. Streamlit Cloud Secrets í™•ì¸
2. ë¡œì»¬ .streamlit/secrets.toml í™•ì¸  
3. í™˜ê²½ë³€ìˆ˜ `export OPENAI_API_KEY="í‚¤"` ì„¤ì •

#### ë„¤ì´ë²„ API í• ë‹¹ëŸ‰ ì´ˆê³¼
```
Error: Quota exceeded
```
**í•´ê²°ì±…**:
1. [ë„¤ì´ë²„ ê°œë°œìì„¼í„°](https://developers.naver.com) API ì‚¬ìš©ëŸ‰ í™•ì¸
2. RSS ì „ìš© ëª¨ë“œë¡œ ì „í™˜
3. API í‚¤ ì—…ê·¸ë ˆì´ë“œ

#### LangGraph ìƒíƒœ ì˜¤ë¥˜
```
Error: StateGraph validation failed
```
**í•´ê²°ì±…**:
1. WorkflowState TypedDict í•„ë“œ í™•ì¸
2. ë…¸ë“œ ë°˜í™˜ê°’ íƒ€ì… ê²€ì¦
3. ìƒíƒœ í•„ë“œëª… ì¼ì¹˜ ì—¬ë¶€ í™•ì¸

### ì„±ëŠ¥ ìµœì í™”

#### LLM í˜¸ì¶œ ìµœì í™”
```python
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def cached_analysis(title: str, description: str):
    """ì¤‘ë³µ ë¶„ì„ ë°©ì§€ ìºì‹±"""
    return analyze_article(title, description)
```

#### ë°°ì¹˜ ì²˜ë¦¬
```python
def analyze_articles_batch(self, articles: List[Dict]) -> List[Dict]:
    """ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•œë²ˆì— ë¶„ì„"""
    batch_prompt = "\n---\n".join([
        f"ê¸°ì‚¬ {i+1}: {article['title']}\n{article['description']}"
        for i, article in enumerate(articles)
    ])
    
    response = self.llm.invoke([HumanMessage(content=batch_prompt)])
    return self.parse_batch_response(response.content)
```

### ì—ëŸ¬ í•¸ë“¤ë§
```python
def robust_workflow_execution(self, state: WorkflowState) -> WorkflowState:
    """ê²¬ê³ í•œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    try:
        return self.execute_step(state)
    except Exception as e:
        # ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬
        state["errors"] = state.get("errors", [])
        state["errors"].append(f"Step failed: {str(e)}")
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ê³„ì† ì§„í–‰
        return self.set_default_values(state)
```

## ğŸ“ ê°œë°œ ì§€ì›

### ë¬¸ì˜ ë° ê¸°ì—¬
- **Issues**: GitHub Issuesì—ì„œ ë²„ê·¸ ë¦¬í¬íŠ¸
- **Pull Requests**: ì½”ë“œ ê¸°ì—¬ëŠ” PRë¡œ ì œì¶œ
- **Documentation**: docs/ í´ë”ì— ì¶”ê°€ ë¬¸ì„œ ì‘ì„±

### ë¼ì´ì„¼ìŠ¤
MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬ ê°€ëŠ¥

---

**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ ì „ì²´ ì‹œìŠ¤í…œì„ ì´í•´í•˜ê³  í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

*ì´ ê°€ì´ë“œëŠ” í”„ë¡œì íŠ¸ ë°œì „ê³¼ í•¨ê»˜ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.* 