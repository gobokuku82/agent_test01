import requests
import feedparser
from bs4 import BeautifulSoup
import streamlit as st
from typing import List, Dict, Any
import time
import re
from urllib.parse import urljoin, urlparse

class EnhancedNewsAPI:
    def __init__(self):
        try:
            self.naver_client_id = st.secrets["NAVER_CLIENT_ID"]
            self.naver_client_secret = st.secrets["NAVER_CLIENT_SECRET"]
        except KeyError:
            self.naver_client_id = None
            self.naver_client_secret = None
        
        # ì–¸ë¡ ì‚¬ë³„ RSS í”¼ë“œì™€ ì›¹ì‚¬ì´íŠ¸ ì •ë³´
        self.media_sources = {
            'ì¡°ì„ ì¼ë³´': {
                'rss': 'https://www.chosun.com/arc/outboundfeeds/rss/',
                'website': 'https://www.chosun.com',
                'search_url': 'https://www.chosun.com/search/?query={keyword}',
                'keywords': ['ì¡°ì„ ì¼ë³´', 'chosun', 'ì¡°ì„ ']
            },
            'ë™ì•„ì¼ë³´': {
                'rss': 'https://rss.donga.com/total.xml',
                'website': 'https://www.donga.com',
                'search_url': 'https://www.donga.com/news/search?query={keyword}',
                'keywords': ['ë™ì•„ì¼ë³´', 'donga', 'ë™ì•„']
            },
            'ì¤‘ì•™ì¼ë³´': {
                'rss': 'https://rss.joins.com/joins_news_list.xml',
                'website': 'https://www.joongang.co.kr',
                'search_url': 'https://www.joongang.co.kr/search/{keyword}',
                'keywords': ['ì¤‘ì•™ì¼ë³´', 'joongang', 'joins', 'ì¤‘ì•™']
            },
            'í•œê²¨ë ˆ': {
                'rss': 'http://feeds.hani.co.kr/rss/newsstand/',
                'website': 'https://www.hani.co.kr',
                'search_url': 'https://www.hani.co.kr/arti/SEARCH/{keyword}',
                'keywords': ['í•œê²¨ë ˆ', 'hani', 'ê²½í–¥']
            },
            'ê²½í–¥ì‹ ë¬¸': {
                'rss': 'http://rss.khan.co.kr/rss.xml',
                'website': 'https://www.khan.co.kr',
                'search_url': 'https://www.khan.co.kr/search/{keyword}',
                'keywords': ['ê²½í–¥ì‹ ë¬¸', 'khan', 'ê²½í–¥']
            },
            'SBS': {
                'rss': 'https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01',
                'website': 'https://news.sbs.co.kr',
                'search_url': 'https://news.sbs.co.kr/news/search/main.do?query={keyword}',
                'keywords': ['SBS', 'sbs']
            },
            'MBC': {
                'rss': 'https://imnews.imbc.com/rss/news/news_00.xml',
                'website': 'https://imnews.imbc.com',
                'search_url': 'https://imnews.imbc.com/search/{keyword}',
                'keywords': ['MBC', 'mbc', 'imbc']
            },
            'KBS': {
                'rss': 'http://world.kbs.co.kr/rss/rss_news.htm',
                'website': 'https://news.kbs.co.kr',
                'search_url': 'https://news.kbs.co.kr/search/{keyword}',
                'keywords': ['KBS', 'kbs']
            }
        }

    def collect_articles_hybrid(self, keyword: str, publishers: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """
        ë„¤ì´ë²„ API + RSS + ì›¹ ìŠ¤í¬ë˜í•‘ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê¸°ì‚¬ ìˆ˜ì§‘
        """
        all_articles = {}
        
        print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ í•˜ì´ë¸Œë¦¬ë“œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘")
        
        # 1. ë„¤ì´ë²„ APIë¡œ ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘
        naver_articles = self._get_naver_articles(keyword)
        naver_filtered = self._filter_naver_articles(naver_articles, publishers)
        
        # 2. RSS í”¼ë“œì—ì„œ ì¶”ê°€ ìˆ˜ì§‘
        for publisher in publishers:
            print(f"ğŸ“° {publisher} ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
            
            articles = []
            
            # ë„¤ì´ë²„ API ê²°ê³¼ ì¶”ê°€
            if publisher in naver_filtered:
                articles.extend(naver_filtered[publisher])
            
            # RSS í”¼ë“œì—ì„œ ì¶”ê°€ ìˆ˜ì§‘
            rss_articles = self._get_rss_articles(publisher, keyword)
            articles.extend(rss_articles)
            
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 3ê°œ ì„ íƒ
            unique_articles = self._remove_duplicates(articles)
            all_articles[publisher] = unique_articles[:3]
            
            print(f"  -> {len(all_articles[publisher])}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´
            time.sleep(1)
        
        return all_articles

    def _get_naver_articles(self, keyword: str) -> List[Dict[str, Any]]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ APIì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        if not self.naver_client_id or not self.naver_client_secret:
            print("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ì–´ RSS í”¼ë“œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return []
        
        headers = {
            'X-Naver-Client-Id': self.naver_client_id,
            'X-Naver-Client-Secret': self.naver_client_secret
        }
        
        params = {
            'query': keyword,
            'display': 100,  # ë” ë§ì€ ê¸°ì‚¬ ìš”ì²­
            'start': 1,
            'sort': 'date'
        }
        
        try:
            response = requests.get(
                "https://openapi.naver.com/v1/search/news.json",
                headers=headers,
                params=params,
                timeout=10
            )
            
            if response.status_code == 200:
                articles = response.json().get('items', [])
                print(f"ğŸ“Š ë„¤ì´ë²„ API: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                return articles
            else:
                print(f"âŒ ë„¤ì´ë²„ API ì˜¤ë¥˜: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _filter_naver_articles(self, articles: List[Dict[str, Any]], publishers: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """ë„¤ì´ë²„ API ê²°ê³¼ë¥¼ ì–¸ë¡ ì‚¬ë³„ë¡œ í•„í„°ë§ (ê°œì„ ëœ ë§¤ì¹­)"""
        filtered = {pub: [] for pub in publishers}
        
        for article in articles:
            title = self._clean_html(article.get('title', ''))
            description = self._clean_html(article.get('description', ''))
            link = article.get('originallink', article.get('link', ''))
            
            # ê° ì–¸ë¡ ì‚¬ì˜ í‚¤ì›Œë“œë¡œ ë§¤ì¹­ ì‹œë„
            for publisher in publishers:
                if publisher in self.media_sources:
                    keywords = self.media_sources[publisher]['keywords']
                    
                    # ì œëª©, ì„¤ëª…, ë§í¬ì—ì„œ ì–¸ë¡ ì‚¬ í‚¤ì›Œë“œ ê²€ìƒ‰
                    if any(keyword.lower() in title.lower() or 
                           keyword.lower() in description.lower() or 
                           keyword.lower() in link.lower() 
                           for keyword in keywords):
                        
                        filtered[publisher].append({
                            'title': title,
                            'description': description,
                            'link': link,
                            'pubDate': article.get('pubDate', ''),
                            'publisher': publisher,
                            'source': 'naver_api'
                        })
                        break
        
        return filtered

    def _get_rss_articles(self, publisher: str, keyword: str) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        if publisher not in self.media_sources:
            return []
        
        rss_url = self.media_sources[publisher]['rss']
        articles = []
        
        try:
            # RSS í”¼ë“œ íŒŒì‹±
            response = requests.get(rss_url, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                
                for entry in feed.entries[:20]:  # ìµœê·¼ 20ê°œ í•­ëª©ë§Œ í™•ì¸
                    title = entry.get('title', '')
                    description = entry.get('description', '') or entry.get('summary', '')
                    link = entry.get('link', '')
                    pub_date = entry.get('published', '')
                    
                    # í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ì„¤ëª…ì— í¬í•¨ëœ ê²½ìš°ë§Œ ì„ íƒ
                    if (keyword.lower() in title.lower() or 
                        keyword.lower() in description.lower()):
                        
                        articles.append({
                            'title': self._clean_html(title),
                            'description': self._clean_html(description)[:300] + '...',
                            'link': link,
                            'pubDate': pub_date,
                            'publisher': publisher,
                            'source': 'rss_feed'
                        })
                
                print(f"  ğŸ“¡ RSS í”¼ë“œ: {len(articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬")
                
        except Exception as e:
            print(f"  âŒ RSS í”¼ë“œ ì˜¤ë¥˜ ({publisher}): {e}")
        
        return articles

    def _remove_duplicates(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ê¸°ì‚¬ ì œê±°"""
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            title = article['title'].lower().strip()
            # ì œëª©ì˜ ì²« 30ìë¡œ ì¤‘ë³µ íŒë‹¨
            title_key = title[:30]
            
            if title_key not in seen_titles and len(title) > 10:
                seen_titles.add(title_key)
                unique_articles.append(article)
        
        return unique_articles

    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±°"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = text.replace('&quot;', '"').replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        text = text.replace('&nbsp;', ' ').replace('&#39;', "'")
        
        return text.strip()

    def get_sample_articles(self, publishers: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """
        í‚¤ì›Œë“œ ì—†ì´ ê° ì–¸ë¡ ì‚¬ì˜ ìµœì‹  ê¸°ì‚¬ ìƒ˜í”Œ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸ìš©)
        """
        sample_articles = {}
        
        for publisher in publishers:
            if publisher not in self.media_sources:
                continue
                
            print(f"ğŸ“° {publisher} ìµœì‹  ê¸°ì‚¬ ìƒ˜í”Œ ìˆ˜ì§‘ ì¤‘...")
            
            try:
                rss_url = self.media_sources[publisher]['rss']
                response = requests.get(rss_url, timeout=10)
                
                if response.status_code == 200:
                    feed = feedparser.parse(response.content)
                    articles = []
                    
                    for entry in feed.entries[:3]:  # ìµœì‹  3ê°œë§Œ
                        articles.append({
                            'title': self._clean_html(entry.get('title', '')),
                            'description': self._clean_html(entry.get('description', '') or entry.get('summary', ''))[:200] + '...',
                            'link': entry.get('link', ''),
                            'pubDate': entry.get('published', ''),
                            'publisher': publisher,
                            'source': 'rss_sample'
                        })
                    
                    sample_articles[publisher] = articles
                    print(f"  âœ… {len(articles)}ê°œ ìƒ˜í”Œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    
            except Exception as e:
                print(f"  âŒ {publisher} ìƒ˜í”Œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                sample_articles[publisher] = []
            
            time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        
        return sample_articles

# ê¸°ì¡´ NewsWorkflowNodesì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í—¬í¼ í•¨ìˆ˜ ì œê³µ
def get_enhanced_news_api():
    """EnhancedNewsAPI ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return EnhancedNewsAPI() 